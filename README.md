# EduCO ğŸ¤–

Low-cost robotics for schools and NGOsâ€”now with a full Python control stack, live camera streaming, and dataset-ready move logging.

![Demo](educo_2.png)

## ğŸ“š Table of Contents
- [Mission](#mission)
- [Current Demo](#current-demo)
- [Quick Start](#quick-start)
- [Hardware Bill of Materials](#hardware-bill-of-materials)
- [Feature Tracker](#feature-tracker)
- [What Happens When You Move a Servo?](#what-happens-when-you-move-a-servo)
- [Live Roadmap](#live-roadmap)
- [Data Capture for ML Training](#data-capture-for-ml-training)
- [Credits](#credits)
- [How to Help](#how-to-help)

## ğŸ¯ Mission
Build an ultra-cheap educational robotics platform controllable by SBCs (Orange Pi, Raspberry Pi, ESP32, Arduinoâ€¦). The idea was born at the Hugging Face LeRobot Hackathon 2025 in SÃ£oÂ Paulo and is still expanding.

> ğŸ¥ Want a quick overview? Watch the [about video](https://github.com/hugorteixeira/educo/raw/refs/heads/main/about_educo.mp4).

## ğŸ“¹ Current Demo
Basic arm movement driven by an Arduino + potentiometer pair. See it in action in the [video demo](https://github.com/hugorteixeira/educo/raw/refs/heads/main/demo_arduino.mp4).

![Demo](thumb.png)

## âš¡ Quick Start
1. **Install dependencies**
   ```bash
   pip install -r robot_api/app/requirements.txt
   ```
2. **Launch the control stack**
   ```bash
   ./run_robot.sh            # Python UI at http://<host>:<port>/ui
   ./run_robot.sh --api-only # API only, no web UI
   ./run_robot.sh --ui none  # explicit UI disable
   ```
3. **Toggle â€œLog movesâ€ in the UI** to capture pre/post snapshots + JSONL entries for ML datasets.

ğŸ’¡ Permissions: if you use software PWM (`servo_driver = soft`), add your user to the `gpio` group so libgpiod can access `/dev/gpiochip*`.

## ğŸ§° Hardware Bill of Materials (â‰ˆÂ $42 on AliExpress)
- Generic arm: ~$16
- ESP32 Cam MB: ~$8
- 4Ã— SG90 servos: ~$4
- Arduino Nano: ~$4
- 4Ã— potentiometers: ~$4
- Micro-USB PSU/DIP adapter: ~$3
- Protoboard: <$2
- Jumpers: <$2

## âœ… Feature Tracker
| Area | Status |
|------|--------|
| Basic Arduino movement | âœ… Complete |
| Orange Pi PWM optimisation | âœ… Complete |
| Custom Orange Pi demo | âœ… Complete |
| ESP32-CAM integration | âœ… Complete |
| REST control API (FastAPI) | âœ… Complete |
| Web UI (Python, neon theme, WASD controls) | âœ… Complete |
| Shiny R UI | ğŸ—ƒï¸ Archived (see `deprecated/`) |
| Move logging with snapshots | âœ… Complete |
| AI vision loop | ğŸ”„ Needs polish |
| VLA model fine-tuning | ğŸ”œ Planned |
| Hugging Face Spaces deployment | ğŸ”œ Planned |

## ğŸ§  What Happens When You Move a Servo?
```mermaid
graph TD
    UI[Web UI / API Client] -->|POST /servo/move| FastAPI
    FastAPI --> Controller[ServoController]
    Controller --> Backend((PWM Backend))
    Backend --> Servos
    FastAPI -->|if logging enabled| Logger
    Logger --> Frames[/Frames/logs/*.jpg/]
    Logger --> Dataset[(move_logs.jsonl)]
```

## ğŸ—ºï¸ Live Roadmap
### Recently Landed
- âœ¨ Python-only launcher (`run_robot.sh`) with selectable UI mode.
- âœ¨ FastAPI UI with keyboard/touch control, neon styling, config form.
- âœ¨ Config edits for camera, per-part step size, and servo ranges directly from the browser.
- âœ¨ Move logging: pre/post camera snapshots + structured JSONL entries for ML training.

### Up Next
- ğŸ” Integrate AI-assisted vision loop (autonomous reactions to camera frames).
- ğŸ§  Start fine-tuning a VLA model using the new move/image logs.
- â˜ï¸ Publish a public demo (Hugging Face Space or lightweight cloud instance).
- ğŸ› ï¸ Add calibration helpers (auto range detection, servo diagnostics).

### Longer Term Ideas
- ğŸ“¦ Modular lesson plans + worksheets.
- ğŸª› Swappable hardware profiles (Jetson Nano, Raspberry Pi 5, etc.).
- ğŸŒ Multi-user remote sessions with permissions.

## ğŸ§¾ Data Capture for ML Training
Toggle the â€œLog movesâ€ switch in the UI to record each servo move:

- **Structured entry** (`logs/move_logs.jsonl`): pin, part, requested angle, servo status before/after, and links to snapshots.
- **Snapshots** (`frames/logs/<id>.jpg`): taken immediately before and after the move.
- Dataset-friendly: combine JSONL + images for behaviour cloning or VLA training.

```json
{
  "id": "8a5b8c...",
  "timestamp": "2025-03-18T02:44:19Z",
  "pin": 31,
  "part": "claw",
  "target_value": -20,
  "smooth": false,
  "status_before": [...],
  "status_after": [...],
  "camera": {
    "pre_image": "/frames/logs/pre_...jpg",
    "post_image": "/frames/logs/post_...jpg"
  }
}
```

## ğŸ™Œ Credits
Project kicked off at the Hugging Face LeRobot Hackathon 2025 in SÃ£oÂ Paulo. Huge thanks to everyone experimenting with ultra-low-cost robotics!

## ğŸ¤ How to Help
- ğŸ§ª Test on different SBCs (Jetson, Raspberry Pi, BananaÂ Piâ€¦)
- ğŸ§· Improve servo control and calibration routines
- ğŸ“¸ Suggest better camera placements / streaming tips
- ğŸ§‘â€ğŸ« Share educational feedback or lesson ideas
- ğŸ§  Join the vision/model fine-tuning effort

> Have ideas or want to walk through the code together? Open an issue or drop a PRâ€”let's make robotics accessible for every classroom.
